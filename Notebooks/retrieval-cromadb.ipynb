{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Assets\\Gen AI\\ChatBot\\myEnv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'query': 'CHAUHAN RIDHAM VIJAYKUMAR Working in Which Company in 2024 According to Placement?', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n\\n\\nQuestion: CHAUHAN RIDHAM VIJAYKUMAR Working in Which Company in 2024 According to Placement?\\nHelpful Answer: CHAUHAN RIDHAM VIJAYKUMAR Working in Which Company in 2024 According to Placement?\\n\\nContext: CHAUHAN RIDHAM VIJAYKUMAR is a well-known Indian politician and a former member of the Indian National Congress. In 2024, he was a candidate for the Rajya Sabha, the upper house of the Indian parliament. He was also a member of the 15th Lok Sabha, the lower house of the Indian parliament\", 'source_documents': []}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "import chromadb\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "#In this demo we will explore using Streamlit to input a question to llm and display the response\n",
    "\n",
    "temperature = 1\n",
    "max_length = 400\n",
    "llm_model = 'tiiuae/falcon-7b-instruct'\n",
    "token = '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'\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=llm_model,\n",
    "    model_kwargs={'temperature': temperature, 'max_length': max_length},\n",
    "    huggingfacehub_api_token=token\n",
    ")\n",
    "\n",
    "#Step 2 - here we connect to a chromadb server. we need to run the chromadb server before we connect to it\n",
    "\n",
    "client = chromadb.HttpClient(host=\"127.0.0.1\")\n",
    "\n",
    "# Setup embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "#Step 4 - here we create a retriever that gets relevant documents (similar in meaning to a query)\n",
    "\n",
    "db = Chroma(client=client, embedding_function=embeddings)\n",
    "\n",
    "retv = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "#Step 5 - here we can explore how similar documents to the query are returned by prining the document metadata. This step is optional\n",
    "\n",
    "docs = retv.get_relevant_documents('PLACEMENT-2024')\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "pretty_print_docs(docs)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "#Step 6 - here we create a retrieval chain that takes llm , retirever objects and invoke it to get a response to our query\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, retriever=retv,chain_type=\"stuff\",return_source_documents=True)\n",
    "\n",
    "response = chain.invoke(\"CHAUHAN RIDHAM VIJAYKUMAR Working in Which Company in 2024 According to Placement?\")\n",
    "\n",
    "print(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
      "1. If you don't know the answer, don't try to make up an answer. Just say \"I can't find the final answer but you may want to check the following links\".\n",
      "2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
      "\n",
      "\n",
      "\n",
      "Question: CHAUHAN RIDHAM VIJAYKUMAR Working in Which Company in 2024 According to Placement?\n",
      "\n",
      "Helpful Answer:\n",
      "\n",
      "Chauhan Ridham Vijay Kumar is currently working at Google as a Senior Software Engineer. He was placed there in 2024.\n"
     ]
    }
   ],
   "source": [
    "# Use similarity searching algorithm and return 3 most relevant documents.\n",
    "client = chromadb.HttpClient(host=\"127.0.0.1\")\n",
    "\n",
    "# Setup embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "db = Chroma(client=client, embedding_function=embeddings)\n",
    "\n",
    "retv = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"tiiuae/falcon-7b-instruct\",\n",
    "    model_kwargs={\"temperature\":1, \"max_length\":500}\n",
    ")\n",
    "\n",
    "query = \"\"\"CHAUHAN RIDHAM VIJAYKUMAR Working in Which Company in 2024 According to Placement?\"\"\"\n",
    "# llm.invoke(query)\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
    "1. If you don't know the answer, don't try to make up an answer. Just say \"I can't find the final answer but you may want to check the following links\".\n",
    "2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    " template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retv,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "# Call the QA chain with our query.\n",
    "result = retrievalQA.invoke({\"query\": query})\n",
    "print(result['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
