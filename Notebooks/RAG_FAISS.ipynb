{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 1: load all documents and create splits/chunks of it(FOR PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before split, there were 111 documents loaded, with average characters equal to 2906.\n",
      "After split, there were 534 documents (chunks), with average characters equal to 603 (average chunk length).\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import numpy as np\n",
    "\n",
    "# Load pdf files in the local directory\n",
    "loader = PyPDFDirectoryLoader(\"./DataDocs/\")\n",
    "\n",
    "docs_before_split = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap  = 50,\n",
    ")\n",
    "docs_after_split = text_splitter.split_documents(docs_before_split)\n",
    "\n",
    "docs_after_split[0]\n",
    "\n",
    "avg_doc_length = lambda docs: sum([len(doc.page_content) for doc in docs])//len(docs)\n",
    "avg_char_before_split = avg_doc_length(docs_before_split)\n",
    "avg_char_after_split = avg_doc_length(docs_after_split)\n",
    "\n",
    "print(f'Before split, there were {len(docs_before_split)} documents loaded, with average characters equal to {avg_char_before_split}.')\n",
    "print(f'After split, there were {len(docs_after_split)} documents (chunks), with average characters equal to {avg_char_after_split} (average chunk length).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 1: load all documents and create splits/chunks of it(FOR CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before split, there were 4350 documents loaded, with average characters equal to 106.\n",
      "After split, there were 4350 documents (chunks), with average characters equal to 106 (average chunk length).\n",
      "L D College of Engineering, Ahmedabad    Placement Status of Batch 2018-19\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to load CSV files from a directory\n",
    "def load_csv_directory(directory):\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Convert each row of the dataframe to a document\n",
    "            for index, row in df.iterrows():\n",
    "                content = row.to_string()\n",
    "                # Wrap the content in a Document object\n",
    "                data.append(Document(page_content=content))\n",
    "    return data\n",
    "\n",
    "# Load CSV files in the local directory\n",
    "directory = \"./DataDocs/CSV/\"\n",
    "docs_before_split = load_csv_directory(directory)\n",
    "\n",
    "# Split documents using RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "docs_after_split = text_splitter.split_documents(docs_before_split)\n",
    "\n",
    "# Function to calculate average document length\n",
    "avg_doc_length = lambda docs: sum([len(doc.page_content) for doc in docs]) // len(docs)\n",
    "\n",
    "# Calculate average characters before and after splitting\n",
    "avg_char_before_split = avg_doc_length(docs_before_split)\n",
    "avg_char_after_split = avg_doc_length(docs_after_split)\n",
    "\n",
    "# Print the results\n",
    "print(f'Before split, there were {len(docs_before_split)} documents loaded, with average characters equal to {avg_char_before_split}.')\n",
    "print(f'After split, there were {len(docs_after_split)} documents (chunks), with average characters equal to {avg_char_after_split} (average chunk length).')\n",
    "\n",
    "# Display the first split document\n",
    "print(docs_after_split[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Let's create embeddings of that chunks and store it in vecotor store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Assets\\Gen AI\\ChatBot\\myEnv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "d:\\Assets\\Gen AI\\ChatBot\\myEnv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ridham\\.cache\\huggingface\\hub\\models--thenlper--gte-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"thenlper/gte-large\",  # alternatively use \"sentence-transformers/all-MiniLM-l6-v2\" for a light and faster experience.\n",
    "    model_kwargs={'device':'cpu'}, \n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "sample_embedding = np.array(huggingface_embeddings.embed_query(docs_after_split[0].page_content))\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs_after_split, huggingface_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check and find data within vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 documents retrieved which are relevant to the query. Display the first one:\n",
      "\n",
      "Branch Of Engineering             Year 2023 Year 2022 Year 2021 Year 2020 Year 2019 Year 2018 Year 2017 Year 2016   Branchwise Total Selection    Information Technology Engineering        128 ...\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"200280116082 IT 2024 RIVEREDGE\"\"\"  \n",
    "         # Sample question, change to other questions you are interested in.\n",
    "relevant_documents = vectorstore.similarity_search(query)\n",
    "print(f'There are {len(relevant_documents)} documents retrieved which are relevant to the query. Display the first one:\\n')\n",
    "print(relevant_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let Make llm with help of HuggingFaceHub llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
      "1. If you don't know the answer, don't try to make up an answer. Just say \"I can't find the final answer but you may want to check the following links\".\n",
      "2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
      "\n",
      "L.D. COLLEGE OF ENGINEERING-BATCH 2024    181   200280116082     CHAUHAN RIDHAM VIJAYKUM...\n",
      "\n",
      "L.D. COLLEGE OF ENGINEERING-BATCH 2024    380   200280119054          CHAUHAN MEET MANOJ...\n",
      "\n",
      "L.D. COLLEGE OF ENGINEERING-BATCH 2024    174   200280116010     CHAUHAN VAIBHAV PRABHUB...\n",
      "\n",
      "Question: According to Placement Data 2024 RIDHAM CHAUHAN Placed in which company?\n",
      "\n",
      "Helpful Answer:\n",
      "\n",
      "Chauhan, Vaibhav, and Manoj are placed in different companies. Vaibhav is placed in TCS, Manoj is placed in Infosys, and Chauhan is placed in L&T.\n"
     ]
    }
   ],
   "source": [
    "# Use similarity searching algorithm and return 3 most relevant documents.\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"tiiuae/falcon-7b-instruct\",\n",
    "    model_kwargs={\"temperature\":0.1, \"max_length\":500}\n",
    ")\n",
    "\n",
    "query = \"\"\"According to Placement Data 2024 RIDHAM CHAUHAN Placed in which company?\"\"\"\n",
    "# llm.invoke(query)\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
    "1. If you don't know the answer, don't try to make up an answer. Just say \"I can't find the final answer but you may want to check the following links\".\n",
    "2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    " template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "# Call the QA chain with our query.\n",
    "result = retrievalQA.invoke({\"query\": query})\n",
    "print(result['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
