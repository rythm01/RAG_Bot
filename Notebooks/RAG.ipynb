{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os, tempfile, glob, random\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown\n",
    "from getpass import getpass\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "# LLM: HuggingFace\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "# langchain prompts, memory, chains...\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from langchain.schema import Document, format_document\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "\n",
    "# Document loaders\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    TextLoader,\n",
    "    DirectoryLoader,\n",
    "    CSVLoader,\n",
    "    UnstructuredExcelLoader,\n",
    "    Docx2txtLoader,\n",
    ")\n",
    "\n",
    "# Text Splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "# OutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Chroma: vectorstore\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "\n",
    "# Contextual Compression\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter,LongContextReorder\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "# Cohere\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain_community.llms import Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Directories: where temp files and vectorstores will be saved\n",
    "\n",
    "TMP_DIR = Path(\"./DataDocs\").resolve().parent.joinpath(\"DataDocs\", \"CSV\")\n",
    "LOCAL_VECTOR_STORE_DIR = Path(\"./db\").resolve().parent.joinpath(\"db\", \"vector_stores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
    "os.environ['COHERE_API_KEY']=\"uRYYzGigTj1i83rqHQck9GZlKgo2Pb5K0AsrpYTx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO]: HUGGINGFACEHUB_API_TOKEN retrieved successfully.\n",
      "\n",
      "[INFO]: COHERE_API_KEY retrieved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Get environment variables: HUGGINGFACEHUB_API_TOKEN, and COHERE_API_KEY\n",
    "\n",
    "def get_environment_variable(key):\n",
    "    if key in os.environ:\n",
    "        value = os.environ.get(key)\n",
    "        print(f\"\\n[INFO]: {key} retrieved successfully.\")\n",
    "    else :\n",
    "        print(f\"\\n[ERROR]: {key} is not found in your environment variables.\") \n",
    "        value = getpass(f\"Insert your {key}\")\n",
    "    return value\n",
    "\n",
    "HF_key = get_environment_variable(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "cohere_api_key = get_environment_variable(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langchain_document_loader(TMP_DIR):\n",
    "    \"\"\"\n",
    "    Load documents from the temporary directory (TMP_DIR). \n",
    "    Files can be in txt, pdf, CSV or docx format.\n",
    "    \"\"\"\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    txt_loader = DirectoryLoader(\n",
    "        TMP_DIR.as_posix(), glob=\"**/*.txt\", loader_cls=TextLoader, show_progress=True\n",
    "    )\n",
    "    documents.extend(txt_loader.load())\n",
    "\n",
    "    pdf_loader = DirectoryLoader(\n",
    "        TMP_DIR.as_posix(), glob=\"**/*.pdf\", loader_cls=PyPDFLoader, show_progress=True\n",
    "    )\n",
    "    documents.extend(pdf_loader.load())\n",
    "\n",
    "    csv_loader = DirectoryLoader(\n",
    "        TMP_DIR.as_posix(), glob=\"**/*.csv\", loader_cls=CSVLoader, show_progress=True,\n",
    "        loader_kwargs={\"encoding\":\"utf8\"}\n",
    "    )\n",
    "    documents.extend(csv_loader.load())\n",
    "\n",
    "    doc_loader = DirectoryLoader(\n",
    "        TMP_DIR.as_posix(),\n",
    "        glob=\"**/*.docx\",\n",
    "        loader_cls=Docx2txtLoader,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    documents.extend(doc_loader.load())\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 30.77it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of documents: 4350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "\n",
    "documents = langchain_document_loader(TMP_DIR)\n",
    "print(f\"\\nNumber of documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Document[3612]** \n",
       "\n",
       " **Page content** (first 1000 character):\n",
       "\n",
       "L.D. COLLEGE OF ENGINEERING-BATCH 2024: 167   200280116068           KHOJA NAVIZ ASHRAFALI      IT   2024         INCUBYTE ...\n",
       "\n",
       "**Metadata:**\n",
       "\n",
       "{'source': 'D:\\\\Assets\\\\Gen AI\\\\ChatBot\\\\DataDocs\\\\CSV\\\\PLACEMENT-2024.csv', 'row': 173}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a random document\n",
    "\n",
    "import random\n",
    "random_document_id = random.choice(range(len(documents)))\n",
    "\n",
    "Markdown(f\"**Document[{random_document_id}]** \\n\\n **Page content** (first 1000 character):\\n\\n\" +\\\n",
    "         documents[random_document_id].page_content[0:1000] + \" ...\"  +\\\n",
    "         \"\\n\\n**Metadata:**\\n\\n\" + str(documents[random_document_id].metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chunks: 4350\n"
     ]
    }
   ],
   "source": [
    "# Create a RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \", \"\"],    \n",
    "    chunk_size = 1600,\n",
    "    chunk_overlap= 200\n",
    ")\n",
    "\n",
    "# Text splitting\n",
    "chunks = text_splitter.split_documents(documents=documents)\n",
    "print(f\"number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarty of sentences (0, 1): 0.793\n",
      "Similarty of sentences (0, 2): 0.742\n",
      "Similarty of sentences (1, 2): 0.772\n"
     ]
    }
   ],
   "source": [
    "def select_embeddings_model(LLM_service=\"OpenAI\"):\n",
    "    \"\"\"Connect to the embeddings API endpoint by specifying the name of the embedding model.\"\"\"\n",
    "\n",
    "    if LLM_service == \"HuggingFace\":\n",
    "        embeddings = HuggingFaceInferenceAPIEmbeddings(    \n",
    "            api_key=HF_key, \n",
    "            model_name=\"thenlper/gte-large\"\n",
    "        )\n",
    "         \n",
    "    return embeddings\n",
    "   \n",
    "\n",
    "embeddings_HuggingFace = select_embeddings_model(LLM_service=\"HuggingFace\")\n",
    "\n",
    "sentences = [\"DATA\",\n",
    "             \"2012\",\n",
    "             \"CHAUHAN RIDHAM VIJAYKUMAR\"]\n",
    "# 1. Calculate embedding vectors\n",
    "embedding_vectors = [embeddings_HuggingFace.embed_query(sentence) for sentence in sentences]\n",
    "\n",
    "for combination in list(combinations(range(len(sentences)),2)):\n",
    "    # 2. Calculate similarity using dot product from numpy:\n",
    "    dot_prodduct = round(np.dot(embedding_vectors[combination[0]], embedding_vectors[combination[1]]),3)\n",
    "    print(f\"Similarty of sentences {combination}: {dot_prodduct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(embeddings, documents, vectorstore_name):\n",
    "    \"\"\"Create a Chroma vector database.\"\"\"\n",
    "    persist_directory = (LOCAL_VECTOR_STORE_DIR.as_posix() + \"/\" + vectorstore_name)\n",
    "    try:\n",
    "        vector_store = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "create_vectorstores = True # change to True to create vectorstores\n",
    "\n",
    "if create_vectorstores:\n",
    "    vector_store_HF = create_vectorstore(\n",
    "        embeddings=embeddings_HuggingFace,\n",
    "        documents = chunks,\n",
    "        vectorstore_name=\"Vit_All_HF_Embeddings\"\n",
    "    )\n",
    "    # print(\"vector_store_HF:\",vector_store_HF._collection.count(),\"chunks.\")\n",
    "    # print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
